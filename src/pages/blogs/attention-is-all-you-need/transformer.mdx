---
layout: ../../../layouts/BlogPost.astro
title: "Transformer"
description: "Wrapping up the Transformer model"
pubDate: 2025-04-3
tags: ["transformer", "mlx", "python", "transformer encoder", "transformer decoder"]
rank: 9
---

### Transformer

Now we have everything needed to wrap up the Transformer model. The Transformer model class will take in `src_tokens` and `tgt_tokens`, pass them through embedding and postional encoding, encoder and decoder. It will then project decoder output to vocabulary size via a final linear layer and return the logits.

### Code

```python
from typing import Optional
import mlx.core as mx
import mlx.nn as nn
from transformer_decoder import TransformerDecoder
from transformer_encoder import TransformerEncoder
from positional_encoding import positional_encoding


class Transformer(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 512,
        num_heads: int = 8,
        d_ff: int = 2048,
        num_layers: int = 6,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)
        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff)

        self.final_linear = nn.Linear(d_model, vocab_size)

    def __call__(
        self,
        src_tokens: mx.array,
        tgt_tokens: mx.array,
        src_mask: Optional[mx.array] = None,
        tgt_mask: Optional[mx.array] = None,
        enc_dec_mask: Optional[mx.array] = None,
    ):
        src_seq_len = src_tokens.shape[1]
        tgt_seq_len = tgt_tokens.shape[1]
        src_emb = (
            self.embedding(src_tokens)
            + positional_encoding(src_seq_len, self.d_model)[None, :, :]
        )
        tgt_emb = (
            self.embedding(tgt_tokens)
            + positional_encoding(tgt_seq_len, self.d_model)[None, :, :]
        )
        enc_output = self.encoder(src_emb, mask=src_mask)
        dec_output = self.decoder(
            tgt_emb, enc_output, self_mask=tgt_mask, enc_dec_mask=enc_dec_mask
        )
        logits = self.final_linear(dec_output)
        return logits
```
