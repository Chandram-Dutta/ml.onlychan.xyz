---
layout: ../../layouts/BlogPost.astro
title: "Attention Is All You Need"
description: "Notes on my learnings and implementation of this iconic paper from scratch"
pubDate: 2025-04-01
tags: ["transformer", "mlx", "python"]
rank: 2
---

_This is my learning and implementation while reading the paper "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"_

### Contents
- [Input Embedding](./attention-is-all-you-need/input_embedding)
- [Positional Encoding](./attention-is-all-you-need/positional_encoding)
- [Scaled Dot-Product Attention](./attention-is-all-you-need/scaled_dot_product_attention)
- [Multi Head Attention](./attention-is-all-you-need/multi_head_attention)
- [Transformer Encoder Block](./attention-is-all-you-need/transformer_encoder_block)
- [Transformer Encoder](./attention-is-all-you-need/transformer_encoder)
- [Transformer Decoder Block](./attention-is-all-you-need/transformer_decoder_block)
- [Transformer Decoder](./attention-is-all-you-need/transformer_decoder)
- [Look Ahead Mask](./attention-is-all-you-need/look_ahead_mask)
- [Transformer](./attention-is-all-you-need/transformer)

### Citations
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, *30*.
- Hannun, A., Digani, J., Katharopoulos, A., & Collobert, R. (2023). *MLX: Efficient and flexible machine learning on Apple silicon*. Available from https://github.com/ml-explore.
