---
layout: ../../layouts/BlogPost.astro
title: "Attention Is All You Need"
description: "Notes on my learnings and implementation of this iconic paper from scratch"
pubDate: 2025-04-01
tags: ["transformer", "mlx", "python"]
---

> This is my learning and implementation while reading the paper "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"

### Citations
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, *30*.
- Hannun, A., Digani, J., Katharopoulos, A., & Collobert, R. (2023). *MLX: Efficient and flexible machine learning on Apple silicon*. Available from https://github.com/ml-explore.
